{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data - also - Input layer\n",
    "training_X= np.array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "training_Y = np.array([[0],[0], [1], [1]])\n",
    "\n",
    "input_layer_features = 3\n",
    "hidden_layer_nodes = 2\n",
    "output_layer_nodes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Perceptron(object):\n",
    "    def __init__(self):\n",
    "#         self.ilw = np.random.rand(3,4) #setup random weights for input layer\n",
    "        self.hlw = np.random.rand(input_layer_features,hidden_layer_nodes) # setup random weights for hidden layer\n",
    "        self.olw = np.random.rand(hidden_layer_nodes, output_layer_nodes) # setup random weights for output layer\n",
    "        \n",
    "    def sigmoid(self, dot_product):\n",
    "        return 1 / (1 + np.exp(-dot_product))\n",
    "    \n",
    "    def step(self, dot_product):\n",
    "        step_transform = []\n",
    "        for item in dot_product:\n",
    "            if item >= .5:\n",
    "                step_transform.append(1)\n",
    "            elif item < .5:\n",
    "                step_transform.append(0)\n",
    "        return(np.array(step_transform, ndmin=2)) # need ndmin 2 to avoid (4,) we need (4,1)\n",
    "    \n",
    "    def sigmoid_derivative(self, dot_product):\n",
    "        return dot_product * (1 - dot_product)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ptron = Perceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step one get the dot products for the layers\n",
    "\n",
    "# these give us the magnitude of the nodes' reaction to the input from the previous layer\n",
    "# how much stimulation is this neuron receiving from the input - how lit up is it\n",
    "hlr = np.dot(training_X, ptron.hlw) \n",
    "olr = np.dot(hlr, ptron.olw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hlr \n",
      " [[  7.50101915e-04   7.18988488e-01]\n",
      " [  1.28469628e+00   1.46608641e+00]\n",
      " [  4.12473766e-01   8.20038391e-01]\n",
      " [  8.72972617e-01   1.36503651e+00]]\n",
      " olr \n",
      " [[ 0.45063523]\n",
      " [ 1.34487723]\n",
      " [ 0.65061957]\n",
      " [ 1.14489289]]\n"
     ]
    }
   ],
   "source": [
    "print(\"hlr \\n {}\\n olr \\n {}\".format(hlr, olr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 2 - run each layer through it's activation function\n",
    "\n",
    "# this takes the stimulation from above and gives us a classifier to help figure out if the node\n",
    "# is firing the way we want it to.  Similar to logistic regression classification but different\n",
    "# because we are looking at how right it is not just whether it is right or wrong.\n",
    "\n",
    "hlr2 = ptron.sigmoid(hlr)\n",
    "olr2 = ptron.sigmoid(olr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hlr2 \n",
      " [[ 0.50018753  0.67238424]\n",
      " [ 0.78324813  0.81246181]\n",
      " [ 0.60168089  0.69424449]\n",
      " [ 0.70536386  0.79657704]]\n",
      " olr2 \n",
      " [[ 0.61079025]\n",
      " [ 0.79329086]\n",
      " [ 0.65715007]\n",
      " [ 0.75857685]]\n"
     ]
    }
   ],
   "source": [
    "print(\"hlr2 \\n {}\\n olr2 \\n {}\".format(hlr2, olr2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3 - calculate the error\n",
    "# Learning begins by calculating how far of we are from what was experienced and what was desired.\n",
    "# so we calulate the error between expected results and what we calculated\n",
    "\n",
    "error = training_Y - olr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error \n",
      " [[-0.61079025]\n",
      " [-0.79329086]\n",
      " [ 0.34284993]\n",
      " [ 0.24142315]]\n"
     ]
    }
   ],
   "source": [
    "print(\"error \\n {}\".format(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4 get the deltas - how much do we need to learn\n",
    "# we do dot products from right to left through the network\n",
    "# stepping backwards through each calculation to determine a better set of weights\n",
    "# until we find the set that gets the results we are looking for\n",
    "# like the way the human nervous system sends impulses to the brain that then processes and \n",
    "# sends back a response\n",
    "\n",
    "# change in the output layer is calculated by the error\n",
    "# change in the hidden layer is calculated in part by the change in the output layer\n",
    "\n",
    "# talk to Kyle about learning coefficient - multiply by another constant to tune?\n",
    "\n",
    "# sigmoid derivative gives us the instantaneous rate of change or slope at that point on the \n",
    "# sigmoid curve as x approaches 0\n",
    "\n",
    "do = error * ptron.sigmoid_derivative(olr2) # delta=output layer-error times derivitive of output layer\n",
    "\n",
    "# this is trying to multiply a 4x1 by a 4x5 matrix\n",
    "dh = do.dot(olr2.T) #* ptron.sigmoid_derivative(hlr2) \n",
    "\n",
    "# dH = dZ.dot(Wz.T) * sigmoid_(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24999996,  0.22028368],\n",
       "       [ 0.1697705 ,  0.15236762],\n",
       "       [ 0.239661  ,  0.21226908],\n",
       "       [ 0.20782568,  0.16204206]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptron.sigmoid_derivative(hlr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do \n",
      " [[-0.14520043]\n",
      " [-0.13008421]\n",
      " [ 0.07724541]\n",
      " [ 0.04421376]]\n",
      " dh\n",
      " [[-0.08868701 -0.11518617 -0.09541847 -0.11014568]\n",
      " [-0.07945417 -0.10319461 -0.08548485 -0.09867887]\n",
      " [ 0.04718074  0.06127808  0.05076183  0.05859658]\n",
      " [ 0.02700533  0.03507437  0.02905507  0.03353953]]\n"
     ]
    }
   ],
   "source": [
    "print(\"do \\n {}\\n dh\\n {}\".format(do,dh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 5 Update the weights for the next iteration or epoch\n",
    "#  Wz +=  H.T.dot(dZ)                          # update output layer weights\n",
    "ptron.ilw = np.dot(ilr2,olr2.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.2259383 ],\n",
       "       [ 3.01223666],\n",
       "       [ 2.63803963],\n",
       "       [ 2.70841929]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptron.ilw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 4), (4, 1))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "olr2.shape, training_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.51594211,  0.63838857,  0.55177584,  0.6184278 ],\n",
       "        [ 0.7923126 ,  0.78364446,  0.62807226,  0.75307008],\n",
       "        [ 0.74173467,  0.70912423,  0.56106647,  0.71628079],\n",
       "        [ 0.58606164,  0.72397688,  0.61923741,  0.66191659]]),\n",
       " array([[ 0.62349393,  0.55756877,  0.67306074,  0.64910651],\n",
       "        [ 0.82083168,  0.66163391,  0.89604174,  0.93156158],\n",
       "        [ 0.72930755,  0.6033624 ,  0.83683684,  0.87373495],\n",
       "        [ 0.73793922,  0.61831204,  0.77576938,  0.78442603]]),\n",
       " array([1, 1, 1, 1]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ilr2, hlr2, olr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.99121407,  0.32275752,  0.03764298,  0.4432068 ],\n",
       "        [ 0.28391783,  0.39589792,  0.27846427,  0.18896654],\n",
       "        [ 0.06379005,  0.56837692,  0.20784839,  0.48288038]]),\n",
       " array([[ 0.27965337,  0.09262718,  0.40034355,  0.99002406],\n",
       "        [ 0.53417628,  0.06843972,  0.3129677 ,  0.45601461],\n",
       "        [ 0.85431021,  0.63266396,  0.39868224,  0.50902435],\n",
       "        [ 0.01115239,  0.11388817,  0.90244741,  0.38720216]]),\n",
       " array([[ 0.12651864],\n",
       "        [ 0.50352375],\n",
       "        [ 0.26456513],\n",
       "        [ 0.6554159 ]]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptron.ilw, ptron.hlw, ptron.olw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/24560298/python-numpy-valueerror-operands-could-not-be-broadcast-together-with-shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
