{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data - also - Input layer\n",
    "training_X= np.array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "training_Y = np.array([[0],[0], [1], [1]])\n",
    "\n",
    "input_layer_features = 3\n",
    "hidden_layer_nodes = 2\n",
    "output_layer_nodes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Perceptron(object):\n",
    "    def __init__(self):\n",
    "#         self.ilw = np.random.rand(3,4) #setup random weights for input layer\n",
    "        self.hlw = np.random.rand(input_layer_features,hidden_layer_nodes) # setup random weights for hidden layer\n",
    "        self.olw = np.random.rand(hidden_layer_nodes, output_layer_nodes) # setup random weights for output layer\n",
    "        \n",
    "    def sigmoid(self, dot_product):\n",
    "        return 1 / (1 + np.exp(-dot_product))\n",
    "    \n",
    "    def step(self, dot_product):\n",
    "        step_transform = []\n",
    "        for item in dot_product:\n",
    "            if item >= .5:\n",
    "                step_transform.append(1)\n",
    "            elif item < .5:\n",
    "                step_transform.append(0)\n",
    "        return(np.array(step_transform, ndmin=2)) # need ndmin 2 to avoid (4,) we need (4,1)\n",
    "    \n",
    "    def sigmoid_derivative(self, dot_product):\n",
    "        return dot_product * (1 - dot_product)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ptron = Perceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step one get the dot products for the layers\n",
    "\n",
    "# these give us the magnitude of the nodes' reaction to the input from the previous layer\n",
    "# how much stimulation is this neuron receiving from the input - how lit up is it\n",
    "hlr = np.dot(training_X, ptron.hlw) \n",
    "olr = np.dot(hlr, ptron.olw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hlr \n",
      " [[ 0.20589707  0.62397427]\n",
      " [ 0.71133963  1.54444129]\n",
      " [ 0.53710934  1.31309069]\n",
      " [ 0.38012736  0.85532487]]\n",
      " olr \n",
      " [[ 0.57141849]\n",
      " [ 1.55275544]\n",
      " [ 1.27372597]\n",
      " [ 0.85044797]]\n"
     ]
    }
   ],
   "source": [
    "print(\"hlr \\n {}\\n olr \\n {}\".format(hlr, olr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 2 - run each layer through it's activation function\n",
    "\n",
    "# this takes the stimulation from above and gives us a classifier to help figure out if the node\n",
    "# is firing the way we want it to.  Similar to logistic regression classification but different\n",
    "# because we are looking at how right it is not just whether it is right or wrong.\n",
    "\n",
    "hlr2 = ptron.sigmoid(hlr)\n",
    "olr2 = ptron.sigmoid(olr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hlr2 \n",
      " [[ 0.55129319  0.65112189]\n",
      " [ 0.6706971   0.82410943]\n",
      " [ 0.63113972  0.78802988]\n",
      " [ 0.59390382  0.70168296]]\n",
      " olr2 \n",
      " [[ 0.63909042]\n",
      " [ 0.82531135]\n",
      " [ 0.78137991]\n",
      " [ 0.70066111]]\n"
     ]
    }
   ],
   "source": [
    "print(\"hlr2 \\n {}\\n olr2 \\n {}\".format(hlr2, olr2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 3 - calculate the error\n",
    "# Learning begins by calculating how far of we are from what was experienced and what was desired.\n",
    "# so we calulate the error between expected results and what we calculated\n",
    "\n",
    "error = training_Y - olr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error \n",
      " [[-0.63909042]\n",
      " [-0.82531135]\n",
      " [ 0.21862009]\n",
      " [ 0.29933889]]\n"
     ]
    }
   ],
   "source": [
    "print(\"error \\n {}\".format(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,4) (4,2) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-03141e36a661>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# this is trying to multiply a 4x1 by a 4x5 matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mdh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0molr2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mptron\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid_derivative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhlr2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# dH = dZ.dot(Wz.T) * sigmoid_(H)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,4) (4,2) "
     ]
    }
   ],
   "source": [
    "# step 4 get the deltas - how much do we need to learn\n",
    "# we do dot products from right to left through the network\n",
    "# stepping backwards through each calculation to determine a better set of weights\n",
    "# until we find the set that gets the results we are looking for\n",
    "# like the way the human nervous system sends impulses to the brain that then processes and \n",
    "# sends back a response\n",
    "\n",
    "# change in the output layer is calculated by the error\n",
    "# change in the hidden layer is calculated in part by the change in the output layer\n",
    "\n",
    "# talk to Kyle about learning coefficient - multiply by another constant to tune?\n",
    "\n",
    "# sigmoid derivative gives us the instantaneous rate of change or slope at that point on the \n",
    "# sigmoid curve as delta x approaches 0\n",
    "\n",
    "do = error * ptron.sigmoid_derivative(olr2) # delta=output layer-error times derivitive of output layer\n",
    "\n",
    "# this is trying to multiply a 4x1 by a 4x5 matrix\n",
    "dh = do.dot(olr2.T) * ptron.sigmoid_derivative(hlr2) \n",
    "\n",
    "# dH = dZ.dot(Wz.T) * sigmoid_(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.14740867]\n",
      " [-0.11898722]\n",
      " [ 0.03734585]\n",
      " [ 0.06278188]]\n"
     ]
    }
   ],
   "source": [
    "print(do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24736901,  0.22716217],\n",
       "       [ 0.2208625 ,  0.14495308],\n",
       "       [ 0.23280237,  0.16703879],\n",
       "       [ 0.24118207,  0.20932398]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptron.sigmoid_derivative(hlr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do \n",
      " [[-0.14520043]\n",
      " [-0.13008421]\n",
      " [ 0.07724541]\n",
      " [ 0.04421376]]\n",
      " dh\n",
      " [[-0.08868701 -0.11518617 -0.09541847 -0.11014568]\n",
      " [-0.07945417 -0.10319461 -0.08548485 -0.09867887]\n",
      " [ 0.04718074  0.06127808  0.05076183  0.05859658]\n",
      " [ 0.02700533  0.03507437  0.02905507  0.03353953]]\n"
     ]
    }
   ],
   "source": [
    "print(\"do \\n {}\\n dh\\n {}\".format(do,dh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 5 Update the weights for the next iteration or epoch\n",
    "#  Wz +=  H.T.dot(dZ)                          # update output layer weights\n",
    "ptron.ilw = np.dot(ilr2,olr2.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.2259383 ],\n",
       "       [ 3.01223666],\n",
       "       [ 2.63803963],\n",
       "       [ 2.70841929]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptron.ilw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 4), (4, 1))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "olr2.shape, training_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.51594211,  0.63838857,  0.55177584,  0.6184278 ],\n",
       "        [ 0.7923126 ,  0.78364446,  0.62807226,  0.75307008],\n",
       "        [ 0.74173467,  0.70912423,  0.56106647,  0.71628079],\n",
       "        [ 0.58606164,  0.72397688,  0.61923741,  0.66191659]]),\n",
       " array([[ 0.62349393,  0.55756877,  0.67306074,  0.64910651],\n",
       "        [ 0.82083168,  0.66163391,  0.89604174,  0.93156158],\n",
       "        [ 0.72930755,  0.6033624 ,  0.83683684,  0.87373495],\n",
       "        [ 0.73793922,  0.61831204,  0.77576938,  0.78442603]]),\n",
       " array([1, 1, 1, 1]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ilr2, hlr2, olr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.99121407,  0.32275752,  0.03764298,  0.4432068 ],\n",
       "        [ 0.28391783,  0.39589792,  0.27846427,  0.18896654],\n",
       "        [ 0.06379005,  0.56837692,  0.20784839,  0.48288038]]),\n",
       " array([[ 0.27965337,  0.09262718,  0.40034355,  0.99002406],\n",
       "        [ 0.53417628,  0.06843972,  0.3129677 ,  0.45601461],\n",
       "        [ 0.85431021,  0.63266396,  0.39868224,  0.50902435],\n",
       "        [ 0.01115239,  0.11388817,  0.90244741,  0.38720216]]),\n",
       " array([[ 0.12651864],\n",
       "        [ 0.50352375],\n",
       "        [ 0.26456513],\n",
       "        [ 0.6554159 ]]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptron.ilw, ptron.hlw, ptron.olw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/24560298/python-numpy-valueerror-operands-could-not-be-broadcast-together-with-shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
